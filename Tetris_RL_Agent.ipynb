{"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !apt-get install -y libglu1-mesa-dev freeglut3-dev mesa-common-dev\n!pip install -qq gymnasium\n!pip install -qq gymnasium[atari]\n!pip install -qq gymnasium[accept-rom-license]","metadata":{"id":"3lQsPnSaBHI8","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functional package\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# ENV package\nimport gymnasium as gym\n\n# Util package\nimport random\nimport os\nimport json\nfrom tqdm import tqdm\nfrom collections import deque\nfrom typing import Tuple, Deque\n\n# Visualization package\nimport cv2\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom IPython.display import HTML\nfrom tensorflow.keras.utils import plot_model","metadata":{"id":"6yjPLIeXCdVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code for environment","metadata":{"id":"vwBlj5jOICj9"}},{"cell_type":"code","source":"# Create the Tetris environment\nenv = gym.make(\"ALE/Tetris-v5\", frameskip = 4)\n# Get state and action sizes\n# The state size is just an array of pixel value of the game screen\nstate_size = (env.observation_space.shape[0], env.observation_space.shape[1], env.observation_space.shape[2])\naction_size = env.action_space.n","metadata":{"id":"9klPQjDsIAC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_size","metadata":{"id":"Fu6qyn-1qgOR","outputId":"36cd1d84-304e-477f-fd31-e07036db48de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#See what is in a environment step\nenv.reset()","metadata":{"id":"5bdlturZdNKy","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state, reward, done, _, info = env.step(4)\nstate","metadata":{"id":"jyK9WkLIqgOR","outputId":"6ba9dcd9-cdcf-42a6-cdb9-725b7dcf665d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code for Reinforcement Learning Agent","metadata":{"id":"tJ0gJTwFHnR-"}},{"cell_type":"code","source":"# Define the LSTM-based RL model\nclass TetrisLSTMAgent():\n    def __init__(self, state_size: int, action_size: int):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory: Deque[Tuple[np.ndarray, int, float, np.ndarray, bool]] = deque(maxlen=2000)\n        self.gamma = 0.95    # discount rate\n        self.epsilon = 1.0   # exploration rate\n        self.epsilon_min = 0.02\n        self.epsilon_decay = 0.999\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n        self.train_history = {'loss':[]}\n\n    def _build_model(self) -> tf.keras.Model:\n        model = tf.keras.Sequential([\n            layers.Input(shape=(None, *self.state_size)),\n            layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation='relu')),\n            layers.TimeDistributed(layers.MaxPooling2D(2, 2)),\n            layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation='relu')),\n            layers.TimeDistributed(layers.MaxPooling2D(2, 2)),\n            layers.TimeDistributed(tf.keras.layers.Flatten()),\n            layers.LSTM(64, return_sequences=True),\n            layers.LSTM(32),\n            layers.Dense(self.action_size, activation=tf.keras.activations.softmax),\n        ])\n        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n        return model\n\n    def remember(self,\n                 state: np.ndarray,\n                 action: int,\n                 reward: float,\n                 next_state: np.ndarray,\n                 done: bool) -> None:\n        '''\n        Save the experience to the memory\n        '''\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state: np.ndarray):\n        '''\n        The agent return an action based on the state. The action can be the\n        a model prediction or random action base on epsilon (exploration rate)\n        '''\n        # Choose between random action or model action\n        if np.random.rand() <= self.epsilon:\n          return random.randrange(self.action_size)\n        else:\n          state = np.reshape(state, [1, 1, *self.state_size])\n          act_values = self.model.predict(state, verbose=0)\n          return np.argmax(act_values[0])\n\n    def replay(self, batch_size: int):\n        '''\n        Train the model base using the saved memory on the environment\n        '''\n        minibatch = random.sample(self.memory, batch_size)\n        state_batch = []\n        Q_value_batch = []\n        # have the model learning from mini batch by replaying it\n        for state, action, reward, next_state, done in minibatch:\n            # state = np.reshape(state, [1, 1, self.state_size])\n            # next_state = np.reshape(next_state, [1, 1, self.state_size])\n            state_batch.append(state)\n            # set model target as the Q-value from the action\n            target = reward\n            if not done:\n                next_reward = np.amax(\n                               self.model.predict(np.expand_dims(next_state, axis=(0, 1)), verbose=0)[0]\n                          )\n                target = (reward + self.gamma * next_reward)\n            # set the action value to target\n            target_f = self.model.predict(np.expand_dims(state, axis=(0, 1)), verbose=0)[0]\n            target_f[action] = target\n            # Add Q_value to training batch\n            Q_value_batch.append(target_f)\n\n        # train model\n        state_batch = np.array(state_batch)\n        Q_value_batch = np.array(Q_value_batch)\n\n        state_batch = np.expand_dims(state_batch, axis=1)\n        Q_value_batch = np.expand_dims(Q_value_batch, axis=1)\n\n        hist = self.model.fit(state_batch, Q_value_batch, epochs=1, verbose=0, batch_size=batch_size)\n        self.train_history['loss'] += hist.history['loss']\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def plot_model(self, plot_name: str='model.png', save_to: str=''):\n      plot_model(self.model, to_file=os.path.join(save_to, plot_name), show_shapes=True, show_layer_names=True)\n      model_img = plt.imread(plot_name)\n      plt.imshow(model_img)\n      plt.axis('off')\n      plt.figure(figsize=(10, 10))\n      plt.show()","metadata":{"id":"xaJAfIxsCjOh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n# Initialize the agent\ndemo_agent = TetrisLSTMAgent(state_size, action_size)","metadata":{"id":"KOXYKCGRARcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_name = 'model_v1.png'\ndemo_agent.plot_model(plot_name)","metadata":{"id":"5j1x-61ZAoNq","outputId":"1a5e5ef9-8757-4bef-cf55-deb372dc3bb8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the agent","metadata":{"id":"c5k1QdbmIj6c"}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n# Initialize the agent\nagent = TetrisLSTMAgent(state_size, action_size)","metadata":{"id":"tDl8bX_BMpkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training parameters\nEPISODES = 100\nBATCH_SIZE = 100\nEPISODE_MAX_STEPS = 5000\n\n# Main training loop\nfor e in tqdm(range(EPISODES)):\n    env.reset()\n    start_state, reward, done, _ , info= env.step(env.action_space.sample())\n    # state = np.reshape(start_state, [1, 1, state_size])\n    state= start_state\n    print(f'\\n\\nthis is episode {e}')\n    step_counter = 0\n    haft_batch_size = int(BATCH_SIZE/2.)\n    for time in range(EPISODE_MAX_STEPS):  # Limit each episode number of steps\n        #get action and next state with model\n        action = agent.act(state)\n        next_state, reward, done, _ , info= env.step(action)\n        # next_state = np.reshape(next_state, [1, 1, state_size])\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n        if done:\n            print(f\"episode: {e}/{EPISODES}, score: {time}, e: {agent.epsilon}\")\n            break\n    \n        # Train the model using agent memory\n        if len(agent.memory) > BATCH_SIZE and step_counter==haft_batch_size:\n            agent.replay(BATCH_SIZE)\n            step_counter=0\n    \n        step_counter += 1\n        \n    # Save the trained model every episode\n    agent.model.save('tetris_lstm_model.h5')\n    \n    # Log the result after each episode\n    with open('train_log.json', 'w') as f:\n            json.dump(agent.train_history, f)","metadata":{"id":"MxF1sxJUIpI_","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(agent.train_history['loss'])","metadata":{"id":"7i9ErCLBhpJP","outputId":"2edd451e-c30d-4895-d5db-536c3562320b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{"id":"Toyxvnq7qgOS"}},{"cell_type":"markdown","source":"### Download model for testing: <a href=\"https://drive.google.com/drive/folders/1mSn44sBc_bvF4sGPB6Fy4gaaZ9iRv-PM?usp=sharing\">link</a>\n","metadata":{"id":"7w9dPHIJ7Oq_"}},{"cell_type":"code","source":"\n\ndef display_video(frames):\n    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n    orig_backend = matplotlib.get_backend()\n    matplotlib.use('Agg')\n    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    matplotlib.use(orig_backend)\n    ax.set_axis_off()\n    ax.set_aspect('equal')\n    ax.set_position([0, 0, 1, 1])\n    im = ax.imshow(frames[0])\n    def update(frame):\n        im.set_data(frame)\n        return [im]\n    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n                                    interval=50, blit=True, repeat=False)\n    return HTML(anim.to_html5_video())\n\n# Create and wrap the environment\nenv = gym.make(\"ALE/Tetris-v5\", render_mode=\"rgb_array\", frameskip = 4)\nframes = []\ntaken_actions = []\nstates = []\n\n#change this base on where you save the downloaded model\nModel_path = \"tetris_lstm_model.h5\"\n\n# Test the trained model\nenv.reset()\nstate = env.step(env.action_space.sample())[0]\nstate = np.array(state, dtype=float)\nstate = np.reshape(state, [1, 1, *state_size])\nfor t in range(5000):\n\n    action = np.argmax(agent.model.predict(state, verbose=0))\n    taken_actions.append(action)\n\n    next_state, reward, done, _, info = env.step(action)\n    next_state = np.array(next_state)\n    states.append(next_state)\n\n    state = np.reshape(next_state, [1, 1, *state_size])\n\n\n    frames.append(env.render())\n    if done:\n        break\n\nenv.close()\ndisplay_video(frames)","metadata":{"id":"KPLV9NdNexgW","outputId":"f3feebfa-d193-4c82-8484-42427b6d4b38"},"execution_count":null,"outputs":[]}]}